<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Имплементациjа На Трансформер Архитектурата За Македонско-Англиски Превод На Реченици | Денес Нешто Научив</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Имплементациjа На Трансформер Архитектурата За Македонско-Англиски Превод На Реченици" />
<meta name="author" content="Стефан Најдовски, Христијан Горков" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/basics/2025/06/06/transformer-mk-en.html" />
<meta property="og:url" content="http://localhost:4000/basics/2025/06/06/transformer-mk-en.html" />
<meta property="og:site_name" content="Денес Нешто Научив" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-06T09:32:32+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Имплементациjа На Трансформер Архитектурата За Македонско-Англиски Превод На Реченици" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Стефан Најдовски, Христијан Горков"},"dateModified":"2025-06-06T09:32:32+02:00","datePublished":"2025-06-06T09:32:32+02:00","headline":"Имплементациjа На Трансформер Архитектурата За Македонско-Англиски Превод На Реченици","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/basics/2025/06/06/transformer-mk-en.html"},"url":"http://localhost:4000/basics/2025/06/06/transformer-mk-en.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Денес Нешто Научив" /><!-- MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Денес Нешто Научив</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Имплементациjа На Трансформер Архитектурата За Македонско-Англиски Превод На Реченици</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-06-06T09:32:32+02:00" itemprop="datePublished">Jun 6, 2025
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Стефан Најдовски, Христијан Горков</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><br /></p>

<hr />

<p><br /></p>

<p><a href="https://colab.research.google.com/github/najdovski-stefan/Donka-v1/blob/main/Donka_v1_Inference_seq2seq_mk_en-GOOGLE-COLAB.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Run on Colab" /></a></p>

<p><br /></p>

<hr />

<p><br /></p>

<div style="background-color: #ff0000; color: #ffffff; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️  Блогот е во процес на пишување и валидација:</strong>
    <br />
    <br />
        Ве молиме вратете се подоцна

    <br />

    Ви благодариме

</div>

<hr />

<p>Блогов што го читате е резултат на <b>работните групи по вештачка интелигенција</b>, каде што ние студентите <b>Стефан Најдовски</b> и <b>Христијан Горков</b>, запишани на прв циклус студии на <a href="https://fikt.uklo.edu.mk/">Факултетот за Информатички и Комункациски Технологии - Битола</a> под менторство на <a href="https://fikt.uklo.edu.mk/prof-d-r-kostandina-veljanovski/">Проф д-р Костандина Вељановска</a>.
и асистентот <a href="(https://fikt.uklo.edu.mk/darko-pajkovski/)">М-р Дарко Пајковски</a>.</p>

<p>Нашата тема на обработка ќе ви ја претставиме детално, низ илустрации а резулатот од нашето мини истражување е <b>мал модел кој може да преведува македонски текст</b>.</p>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="содржина">Содржина</h1>

<ol>
  <li><a href="#0-вовед">Вовед</a></li>
  <li><a href="#1-токен">Што е токен?</a> (Token)</li>
  <li>Податоци за тренирање (Data Set)</li>
  <li><a href="">Токенизирање</a> (Tokenization)</li>
  <li><a href="">SentencePiece библиотека</a></li>
  <li>Token Eembeddings</li>
  <li>Секвенца во Секвенца</li>
  <li>Лимитации</li>
</ol>

<hr />

<p><br /></p>

<h2 id="1-вовед">1. Вовед</h2>

<p>Целта на овој блог е со <b>двојна природа</b>, <b>првенствено е наменета да научиме како функционира Transformer архитектурата</b>, делот кој ја направи науката за вештачки неуронски мрежи <b>посериозна, практична и достапна за секого</b>, најголема примена има во <a href="https://www.ibm.com/think/topics/large-language-models">Large Language Models</a> како <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, Claude, Mistral, LLAMA…</p>

<p><b>Втората цел ни е да ви претставиме мал модел</b> кој знае да преведува кратки реченици од македонски јазик на англиски јазик.</p>

<p>На крајот има и <b>demo</b> од проектот, што може да се користи практично, со одредени лимитации.</p>

<p>Секако нашата имплементација тука ќе биде <b>многу пати поедноставна </b>, но есенцијално идејата е иста, скоро целосно базирана врз оригиналниот труд <a href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a>.</p>

<p>Целта <b>нема</b> да биди:  Generative Pretrained Transformer (GPT), имплементација на целосен Large Language Model (LLM), е далеку покомплицирано со теорија и уште потешко за имплеметнација, сакавме да бидиме јасни за која е целта.</p>

<p>Она што ќе го имплементираме и објасниме, ќе биди:</p>

<p><b>Sequence to Sequence Vanilla Transformer</b> или на кратко <b>Seq2Seq Transformer</b>.</p>

<p>Ќе гледаме да балансираме со технички жаргони и да објасниме интуитивно и со примери за секој да може да не следи.</p>

<p>Ви препорачуваме да имате некоја блага основа за полесно следење, за тоа што е <b>Neural Network</b> и што е тоа <b>Natural Language Processing</b>, исто препорачливо е познавање на <b>основи на веројатност и статистика</b>.</p>

<p>Доколку сакате да научите или сте љубопитни и жедни за знаење слободно погледнете:
<a href="#корисна-литература-за-почетници-и-за-љубопитните">Корисни ресурси за почетници</a></p>

<p><br /></p>

<div style="background-color: #fff3cd; color: #856404; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️ Мало Предупредување:</strong>

    <br />
    <br />

        Овој блог е пишуван од студенти, сè уште ги проучуваме детално сите идеи презентирани на блогот

  <br />
  <br />

  Сигурни сме дека имаме некаде грешка. Очекувајте грешки како граматички и нестандарден јазик.

  <br />
  <br />

  <b>Отворени сме на конструктивни критики</b>

  <br />
  <br />
  Доколку пронајдите какви било грешки слободно контактирајте не.

  <br />
  <br />
  Благодариме :)
</div>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="1-токен">1. Токен</h2>

<p>Овој збор кај нас би бил преведен како <b>лексема</b> или <b>жетон</b>, во англиската (програмерската) литература e дефиниран како <b>атомична (неделива) единица за репрезентација нa текст.</b> (искрено не сме сигурни дали го имаат истото значење на македонски или општо во лингвистиката).</p>

<p>Должината на оваа единица е <b>произволна</b> и зависи од проблемот што сакаме да го решиме.</p>

<p>Се сретнува во повеќе должини:</p>
<ul>
  <li><b>Реченица</b> (пример: Здраво Македонијо!)</li>
  <li><b>Дел од збор</b> (пример во вистински јазик би биле слоговите).</li>
  <li><b>Збор</b>(пример: <b>здраво</b>).</li>
  <li><b>Карактер</b> (пример: <b>а</b>).</li>
  <li><b>Бајт</b> (пример: <b>ASCII</b> или <b>UTF</b> енкодиран карактер).</li>
</ul>

<p><img src="/assets/images/granularnost.png" alt="granularnost" /></p>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="3речник">3.Речник</h2>

<h2 id="4-токенизирање">4. Токенизирање</h2>

<p>Е процесот на претворање на текст (во нашиот случај македонска кирилица) во токени, со тоа што подоцна истите тие ќе бидат претставени како <b>вектори</b> за моделот да може да ги процесира.</p>

<p>Типови на токенајзери:</p>

<ul>
  <li>
    <p>со правила (Rule-based) (токен-збор,токен-буква), најнеоптимален.</p>
  </li>
  <li>
    <p>Научен (Learned) тип:</p>
  </li>
  <li>
    <p>Во Научените токенизатори спаѓаат: Byte-Pair Encoding (BPE), WordPiece и Униграм.</p>
  </li>
</ul>

<p><img src="/assets/images/tokenizator.png" alt="tokenizator" /></p>

<ul>
  <li>ние го користиме <b>Unigram</b>, со помош на <b>sentencepiece</b> библиотеката.</li>
</ul>

<h2 id="6-dataset">6. Dataset</h2>

<p>За оние кои не се запознаени Data set <b>претставува колекција од податоци</b>, најчесто организирани во табела.</p>

<p>Изглед на нашата “табела”:</p>

<pre><code class="language-tsv">здраво  hello
ние сме студенти.  we are students.
јас сакам да учам.  I want to learn.
...
</code></pre>

<p>За тренирање на нашиот модел ние искористивме корпус кој е достапен на интернет, секако со дозвола на авторите кои можите да ги најдите <a href="#благодарност-до">тука</a>.</p>

<p>Податоците за тренирање ги зачувавме во формат наречен <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000533.shtml">Tab-Separated Value</a> или <b>TSV</b> на кратко, со помош на библиотеката pandas во Python.</p>

<p>Во <b>првата колона ги ставивме речениците на македонски јазик</b>, во <b>втората колона ги ставивме преведените реченици на англиски јазик</b>, дел од речениците беа преведени од почеток, остатокот од другите користевме Google Translate и локални LLM модели со техника на дестилација да враќа формат кој е прифатлив.</p>

<p>Валидација правевме со неколку примероци за квалитетот. Но дефинитивно сметаме дека не е најдобар начин за превод.</p>

<p>Како резултат добивме релативно мал data set од <b>480 илјади преведени реченици</b>.</p>

<p>Дистрибуција според должина на реченици:</p>

<p>Со помош на овие македонско-англиски парови го <a href="2025-06-06-transformer-mk-en#Тренирање:">трениравме моделот</a>.</p>

<h2 id="7-севенца-во-секвенца-sequence-to-sequence">7. Севенца во Секвенца (Sequence to Sequence).</h2>

<p><a href="">Seq2Seq</a> се користи за обработка на природни јазици <a href="https://mk.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%BF%D1%80%D0%B8%D1%80%D0%BE%D0%B4%D0%BD%D0%B8_%D1%98%D0%B0%D0%B7%D0%B8%D1%86%D0%B8">NLP</a>.</p>

<p>Во нашиот случај ќе го користиме за <b>превод од македонски на англиски</b>, мислиме дека е добар баланс помеѓу нешто што е <b> корисно да направиме за нашиот мајчин јазик</b> ,нешто што <b> не е само теорија</b> и нешто што <b>може да се научи</b>, три во едно :)</p>

<p>Пред да се појави трансформер архитектурата, <b>механизмите за “внимание”</b> биле ограничени со <a href="">GRU</a> или <a href="">LSTM</a> и користењето на <a href="">RNN-Recurrent Neural Networks</a>.</p>

<h2 id="8-positional-embeddings">8. Positional embeddings</h2>

<p>Оргиналната имплементација користи статични (фиксни) позициони вградувања.</p>

<p>За да се пресмета вредноста на едео позиционо вградување (3.5 во оригиналното истражувње).</p>

<p>Авторите ги користат функциите <b>синус и косинус</b> (наставниците и професорите беа во право, корисни се) :</p>

\[PE(\text{pos}, 2i) = \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\]

\[PE(\text{pos}, 2i+1) = \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\]

<p>Енкодирањето зависи од 3 вредности:</p>

<ul>
  <li>pos - позицијата на векторот</li>
  <li>i - индексот внатре во векторот</li>
  <li>d_model - димензијата на внесот</li>
</ul>

<p>Позиционалните вградувања се користат за информирање на трансфомерот на која позиција се наоѓаат векторите за внес. Тие се додаваат на секоја вредност во векторот посебно,</p>

<p><img src="/assets/images/visualize.png" alt="granularnost" /></p>

<h2 id="9-внимание">9. Внимание</h2>

<p>Концептот на <b>“Внимание”</b> е да го реши проблемот со преведување на текст, пред 20тина години овој проблем бил решаван со комплексни алгоритми кои имале бројни проблеми, наједноставниот проблем била самата <b>должина на речениците при превод</b>, тие се менуваат и стануваат уште по очигледни кога користиме јазици кои имаат различен начин на пишување, за повеќе околу проблемите од класичните начини на превод без корисење на неуронски мрежи можи да прочитате <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Shortcomings">тука</a>.</p>

<h3 id="клучеви-вредности">Клучеви, Вредности</h3>

\[Vnimanie(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\]

<div style="background-color: #ff0000; color: #ffffff; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️  Предупредување:</strong>
    <br />
    <br />
    Моделот е лиценциран под
    <a href="https://www.creativecommons.org/licenses/by-nc/4.0/deed.en" target="_blank" style="color: #ffffff; text-decoration: underline;">
        Creative Commons Attribution Non Commercial 4.0
    </a>
    <br />
    <br />
    Моделот <b>МОЖЕ</b> да се користи за едукативни цели и истражувачки цели.
    <br />
    <br />
    Моделот <b>НЕ</b> може да биде користен во комерцијални продукти или цели на кој било начин.
</div>

<h2 id="sentencepiece">SentencePiece:</h2>

<h3 id="unigram">Unigram:</h3>

<p>е алгоритам за токенизација на под-зборови, каде што претпоставката е дека појавата на токен е <b>независна</b> од било кој од другите токени кои се појавиле претходно.</p>

<hr />

<p><br /></p>

<h2 id="тренирање">Тренирање:</h2>

<p>Под процесот тренирање се мисли учење на невронската мрежа (трансформерот) да преведува текст.</p>

<p>За овој чекор искористивме <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/">Nvidia графичка RTX 4090</a> со 24 GB VRAM.</p>

<p>Моделот го трениравме 20 епохи, секоја епоха траеше околу еден час, после 20 епохи учење, моделот започна да стагнира.</p>

<p><img src="/assets/images/valtrainlossepoch.png" alt="tokenizator" /></p>

<p>Спред Validation Loss (90% за тренирање, 10% за валидација од Data Set), епоха 18 покажа најдобри резултати.</p>

<p>После епоха 18 има почување на Loss, што покажува знаци на почеток на overfiting?</p>

<p>Кодот за тренирање на моделот може да го најдите тука:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">'is the syntax working?'</span><span class="p">)</span></code></pre></figure>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="резултати">Резултати</h2>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="demo">Demo</h2>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="благодарност-до">Благодарност до:</h2>

<p>Би сакале да изразиме посебна благодарност на следниве личности и институции, кои безусловно ни помогнаа и подржаја, без нив проектов не би постоел, од бесценети ресурси по македонски јазик, од песни, книги па се до вести, нашиот модел не би научил да преведи ниту една реченица, затоа би сакале да им се заблагодариме на следниве личности и институции:</p>

<ul>
  <li><a href="https://fikt.uklo.edu.mk/prof-d-r-kostandina-veljanovski/">Проф д-р Костандина Вељановска - ФИКТ - Битола</a>.</li>
  <li><a href="https://fikt.uklo.edu.mk/darko-pajkovski/">М-р Дарко Пајковски - Асистент - ФИКТ - Битола</a>.</li>
  <li><a href="https://drmj.manu.edu.mk/">Акад. Марјан Марковиќ - Дигитални ресурси на македонскиот јазик</a>.</li>
  <li><a href="https://pelister.org/">Проф. д-р Георге Гоце Митревски - pelister.org</a>.</li>
  <li><a href="https://time.mk/trajkovski/">Ph.D Игор Трајковски - time.mk</a>.</li>
  <li><a href="https://pesna.org/">pesna.org</a>.</li>
</ul>

<p><br /></p>

<hr />

<h3 id="литературареференци">Литература/Референци:</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1706.03762">Attention is All You need</a>.</li>
  <li><a href="https://arxiv.org/pdf/2010.04903">What Do Position Embeddings Learn?</a></li>
  <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>.</li>
  <li><a href="https://erdem.pl/2021/05/introduction-to-attention-mechanism">Kemal Erdem, (May 2021). “Introduction to Attention Mechanism”</a></li>
  <li>[мал дел од dataset - ]</li>
  <li><a href="https://theaisummer.com/positional-embeddings/">theaisummer.com - Positional Embeddings</a></li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="download">Download</h2>

<ul>
  <li><a href="https://huggingface.co/stefan-n/Donka-v1">Модел Donka v1 -  CC BY-NC 4.0 - Hugging Face </a>.</li>
  <li><a href="">Изворен код - Github - Apache</a>.</li>
  <li><a href="">Google Colab - Тестирање</a>.</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h2 id="корисна-литература-за-почетници-и-за-љубопитните">Корисна литература за почетници и за љубопитните:</h2>

<p>Следниве ресурси се комплетно бесплатни и корисни за сите оние кои сакаат да научат кодирање, машинско учење, линеарна алгебра,веројатност и статистика.</p>

<p>Зошто овие теми, бидејќи со помош на овие теми и вие ќе можите да тренирате и да ги разбирате невронските мрежи и вештачката интелигенција.</p>

<ul>
  <li><a href="https://hefferon.net/linearalgebra/">Линеарна Алгебра - Jim Hefferon - книга кој авторот ја нуди бесплатно</a>.</li>
  <li><a href="https://rentry.org/machine-learning-roadmap">Целосен Roadmap за учење</a>.</li>
  <li><a href="https://ocw.mit.edu/courses/6-100l-introduction-to-cs-and-programming-using-python-fall-2022/pages/material-by-lecture/">Научете Python од <b>MIT</b></a>.</li>
  <li><a href="https://codeinplace.stanford.edu/">Научете Python Интеракивно - Курс од <b>Stanford</b></a>.</li>
  <li><a href="https://cs50.harvard.edu/python/2022/weeks/0/">Научете Python - Курс од <b>Harvard</b></a>.</li>
  <li><a href="https://immersivemath.com/ila/index.html">Научете <b>Линеарна Алгебра визуелно</b>, делот со операции за вектори и матрици е многу важен во машинското учење - <b>Immersive Math</b></a>.</li>
  <li><a href="https://playground.tensorflow.org">Визуелно експериментирање со <b> Tensorflow Deep Playground</b></a>.</li>
  <li><a href="https://www.3blue1brown.com/topics/neural-networks"><b>3b1b</b> - Невронски Мрежи</a>.</li>
  <li><a href="https://youtu.be/qBigTkBLU6g?si=oTADX-oND-uB_FVA"><b>StatQuest</b> - Статистика</a>.</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

  </div><a class="u-url" href="/basics/2025/06/06/transformer-mk-en.html" hidden></a>
</article>

      </div>
    </main>

    <!--<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Денес Нешто Научив</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Денес Нешто Научив</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
-->

  </body>

</html>
