<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-06T15:53:39+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Денес Нешто Научив</title><subtitle></subtitle><entry><title type="html">Имплементациjа на Трансформер Архитектурата за Македонско-Англиски Превод На Реченици</title><link href="http://localhost:4000/basics/2025/06/06/transformer-mk-en.html" rel="alternate" type="text/html" title="Имплементациjа на Трансформер Архитектурата за Македонско-Англиски Превод На Реченици" /><published>2025-06-06T09:32:32+02:00</published><updated>2025-06-06T09:32:32+02:00</updated><id>http://localhost:4000/basics/2025/06/06/transformer-mk-en</id><content type="html" xml:base="http://localhost:4000/basics/2025/06/06/transformer-mk-en.html"><![CDATA[<p><br /></p>

<hr />

<h1 id="содржина">Содржина</h1>

<ol>
  <li>Вовед</li>
  <li>Што е токен?</li>
  <li>Речник</li>
  <li>Токенизирање</li>
  <li>SentencePiece</li>
  <li>Token Eembeddings</li>
  <li>Unigram Model</li>
  <li>Position Eembedding</li>
  <li>Внимание</li>
  <li>Transformer</li>
  <li>Multi-Head Attention</li>
  <li>Тренирање</li>
  <li>Резултати</li>
  <li>Референци и Благодарност</li>
</ol>

<hr />

<p><br /></p>

<h2 id="0-вовед">0. Вовед</h2>

<p>Целта на овој блог е да научиме како функционира <b>Transformer</b> архитектурата, делот кој ја направи науката за неуронски мрежи посериозна, се користи во Large Language Models како GPT, Claude, Mistral, LLAMA…</p>

<p>На крајот има и <b>“demo”</b> од проектот, што може да се користи практично.</p>

<p>Секако нашата имплементација тука ќе биде <b>многу пати поедноставна </b>, но есенцијално идејата позади е иста, скоро целосно базирана врз оригиналниот труд <a href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a>.</p>

<p>Целта на нашиот модел ќе биди <b>превод на текст</b>.</p>

<p>Целта <b>нема</b> да биди:  Generative Pretrained Transformer (GPT), имплементација на целосен Large Language Model (LLM), ова е далеку покомплицирано и уште потешко за имплеметнација.</p>

<p>Она што ќе го имплементираме, ќе биди:</p>

<p><b>Sequence to Sequence Vanilla Transformer</b> или на кратко <b>Seq2Seq Transformer</b>.</p>

<p>Ќе гледаме да балансираме со технички жаргони и да објасниме интуитивно и со примери за секој да може да не следи, ви препорачуваме да имате некоја блага основа за полесно следење, за тоа што е <b>Neural Network</b> и што е тоа <b>Natural Language Processing</b>, исто препорачливо е познавање на <b>основи на веројатност и статистика</b>.</p>

<p><br /></p>

<h3 id="sequence-to-sequence">Sequence to Sequence</h3>

<p><a href="">Seq2Seq</a> се користи за обработка на природни јазици <a href="https://mk.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%BF%D1%80%D0%B8%D1%80%D0%BE%D0%B4%D0%BD%D0%B8_%D1%98%D0%B0%D0%B7%D0%B8%D1%86%D0%B8">NLP</a>.</p>

<p>Во нашиот случај ќе го користиме за <b>превод од македонски на англиски</b>, мислиме дека е добар баланс помеѓу нешто што е <b> корисно да направиме за нашиот мајчин јазик</b> ,нешто што <b> не е само теорија</b> и нешто што <b>може да се научи</b>, три во едно :)</p>

<p>Пред да се појави трансформер архитектурата, <b>механизмите за “внимание”</b> биле ограничени со <a href="">GRU</a> или <a href="">LSTM</a> и користењето на <a href="">RNN-Recurrent Neural Networks</a>.</p>

<div style="background-color: #fff3cd; color: #856404; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️ Мало Предупредување:</strong>

    <br />
    <br />

        Овој блог е пишуван од студенти, сеуште ги проучуваме детално сите идеи презентирани на блогот

  <br />
  <br />

  Сигурни сме дека имаме некаде грешка. Очекувајте грешки како граматички и нестандарден јазик.

  <br />
  <br />

  <b>Отворени сме на конструктивни критики</b>

  <br />
  <br />
  Доколку пронајдите какви било грешки слободно контактирајте не.

  <br />
  <br />
  Благодариме :)
</div>

<h2 id="1-токен">1. Токен</h2>

<p>Овој збор кај нас би бил преведен како <b>лексема</b> или <b>жетон</b>, во англиската (програмерската) литература e дефиниран како <b>атомична (неделива) единица за репрезентација нa текст.</b> (искрено не сме сигурни дали го имаат истото значење на македонски или општо во лингвистиката).</p>

<p>Должината на оваа единица е <b>произволна</b> и зависи од проблемот што сакаме да го решиме.</p>

<p>Се сретнува во повеќе должини:</p>
<ul>
  <li><b>Реченица</b> (пример: Здраво Македонијо!)</li>
  <li><b>Дел од збор</b> (пример во вистински јазик би биле слоговите).</li>
  <li><b>Збор</b>(пример: <b>здраво</b>).</li>
  <li><b>Карактер</b> (пример: <b>а</b>).</li>
  <li><b>Бајт</b> (пример: <b>ASCII</b> или <b>UTF</b> енкодиран карактер).</li>
</ul>

<p>Ние ќе ги користиме Unigram Моделот.</p>

<p><br /></p>

<h2 id="3речник">3.Речник</h2>

<h2 id="9внимание">9.Внимание</h2>

<p>Концептот на <b>“Внимание”</b> е да го реши проблемот со преведување на текст, пред 20тина години овој проблем бил решаван со комплексни алгоритми кои имале бројни проблеми, наједноставниот проблем била самата <b>должина на речениците при превод</b>, тие се менуваат и стануваат уште по очигледни кога користиме јазици кои имаат различен начин на пишување, за повеќе околу проблемите од класичните начини на превод без корисење на неуронски мрежи можи да прочитате <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Shortcomings">тука</a>.</p>

<div style="background-color: #ff0000; color: #ffffff; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️  Предупредување:</strong>
    <br />
    Моделот е лиценциран под
    <a href="https://www.creativecommons.org/licenses/by-nc/4.0/deed.en" target="_blank" style="color: #ffffff; text-decoration: underline;">
        Creative Commons Attribution Non Commercial 4.0
    </a>.
    <br />
    <br />
    Може да се користи за едукативни цели и истражувачки цели.
    <br />
    <br />
    Моделот <b>НЕ</b> може да биде користен во комерцијални продукти или цели на кој било начин.
</div>

<!--

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">'is the syntax working?'</span><span class="p">)</span></code></pre></figure>
 -->]]></content><author><name></name></author><category term="basics" /><summary type="html"><![CDATA[]]></summary></entry></feed>