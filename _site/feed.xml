<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-08T12:42:05+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Денес Нешто Научив</title><subtitle></subtitle><entry><title type="html">Имплементациjа На Трансформер Архитектурата За Македонско-Англиски Превод На Реченици</title><link href="http://localhost:4000/basics/2025/06/06/transformer-mk-en.html" rel="alternate" type="text/html" title="Имплементациjа На Трансформер Архитектурата За Македонско-Англиски Превод На Реченици" /><published>2025-06-06T09:32:32+02:00</published><updated>2025-06-06T09:32:32+02:00</updated><id>http://localhost:4000/basics/2025/06/06/transformer-mk-en</id><content type="html" xml:base="http://localhost:4000/basics/2025/06/06/transformer-mk-en.html"><![CDATA[<p><br /></p>

<hr />

<h1 id="содржина">Содржина</h1>

<ol>
  <li><a href="#0-вовед">Вовед</a></li>
  <li><a href="#1-токен">Што е токен?</a></li>
  <li>Речник</li>
  <li>Токенизирање</li>
  <li>SentencePiece</li>
  <li>Token Eembeddings</li>
  <li>Dataset</li>
  <li>Unigram Model</li>
  <li>Positional Embeddings</li>
  <li>Внимание</li>
  <li>Transformer</li>
  <li>Multi-Head Attention</li>
  <li>Тренирање</li>
  <li>Резултати</li>
  <li>Референци и Благодарност</li>
</ol>

<hr />

<p><br /></p>

<h2 id="0-вовед">0. Вовед</h2>

<p>Целта на овој блог е со <b>двојна природа</b>, <b>првенствено е наменета да научиме како функционира Transformer</b> архитектурата, делот кој ја направи науката за вештачки неуронски мрежи посериозна, практична и достапна за секого, најголема примена има во Large Language Models како GPT, Claude, Mistral, LLAMA…</p>

<p><b>Втората цел ни е да ви претставиме мал модел</b> кој знае да преведува кратки реченици од македонски јазик на англиски јазик.</p>

<p>На крајот има и <b>“demo”</b> од проектот, што може да се користи практично.</p>

<p>Секако нашата имплементација тука ќе биде <b>многу пати поедноставна </b>, но есенцијално идејата е иста, скоро целосно базирана врз оригиналниот труд <a href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a>.</p>

<p>Целта <b>нема</b> да биди:  Generative Pretrained Transformer (GPT), имплементација на целосен Large Language Model (LLM), е далеку покомплицирано со теорија и уште потешко за имплеметнација, сакавме да бидиме јасни за која е целта.</p>

<p>Она што ќе го имплементираме и објасниме, ќе биди:</p>

<p><b>Sequence to Sequence Vanilla Transformer</b> или на кратко <b>Seq2Seq Transformer</b>.</p>

<p>Ќе гледаме да балансираме со технички жаргони и да објасниме интуитивно и со примери за секој да може да не следи.</p>

<p>Ви препорачуваме да имате некоја блага основа за полесно следење, за тоа што е <b>Neural Network</b> и што е тоа <b>Natural Language Processing</b>, исто препорачливо е познавање на <b>основи на веројатност и статистика</b>.</p>

<p>Доколку сакате да научите или сте љубопитни и жедни за знаење слободно погледнете:
<a href="#корисна-литература-за-почетници-и-за-љубопитните">Корисни ресурси за почетници</a></p>

<p><br /></p>

<h3 id="sequence-to-sequence">Sequence to Sequence</h3>

<p><a href="">Seq2Seq</a> се користи за обработка на природни јазици <a href="https://mk.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%BF%D1%80%D0%B8%D1%80%D0%BE%D0%B4%D0%BD%D0%B8_%D1%98%D0%B0%D0%B7%D0%B8%D1%86%D0%B8">NLP</a>.</p>

<p>Во нашиот случај ќе го користиме за <b>превод од македонски на англиски</b>, мислиме дека е добар баланс помеѓу нешто што е <b> корисно да направиме за нашиот мајчин јазик</b> ,нешто што <b> не е само теорија</b> и нешто што <b>може да се научи</b>, три во едно :)</p>

<p>Пред да се појави трансформер архитектурата, <b>механизмите за “внимание”</b> биле ограничени со <a href="">GRU</a> или <a href="">LSTM</a> и користењето на <a href="">RNN-Recurrent Neural Networks</a>.</p>

<div style="background-color: #fff3cd; color: #856404; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️ Мало Предупредување:</strong>

    <br />
    <br />

        Овој блог е пишуван од студенти, сè уште ги проучуваме детално сите идеи презентирани на блогот

  <br />
  <br />

  Сигурни сме дека имаме некаде грешка. Очекувајте грешки како граматички и нестандарден јазик.

  <br />
  <br />

  <b>Отворени сме на конструктивни критики</b>

  <br />
  <br />
  Доколку пронајдите какви било грешки слободно контактирајте не.

  <br />
  <br />
  Благодариме :)
</div>

<h2 id="1-токен">1. Токен</h2>

<p>Овој збор кај нас би бил преведен како <b>лексема</b> или <b>жетон</b>, во англиската (програмерската) литература e дефиниран како <b>атомична (неделива) единица за репрезентација нa текст.</b> (искрено не сме сигурни дали го имаат истото значење на македонски или општо во лингвистиката).</p>

<p>Должината на оваа единица е <b>произволна</b> и зависи од проблемот што сакаме да го решиме.</p>

<p>Се сретнува во повеќе должини:</p>
<ul>
  <li><b>Реченица</b> (пример: Здраво Македонијо!)</li>
  <li><b>Дел од збор</b> (пример во вистински јазик би биле слоговите).</li>
  <li><b>Збор</b>(пример: <b>здраво</b>).</li>
  <li><b>Карактер</b> (пример: <b>а</b>).</li>
  <li><b>Бајт</b> (пример: <b>ASCII</b> или <b>UTF</b> енкодиран карактер).</li>
</ul>

<p>Ние ќе ги користиме Unigram Моделот.</p>

<p><br /></p>

<h2 id="3речник">3.Речник</h2>

<h2 id="6-dataset">6. Dataset</h2>

<p>За оние кои не се запознаени Data set <b>претставува колекција од податоци</b>, најчесто организирани во табела.</p>

<p>Изглед на нашата “табела”:</p>

<pre><code class="language-tsv">здраво  hello
ние сме студенти.  we are students.
јас сакам да учам.  I want to learn.
...
</code></pre>

<p>За тренирање на нашиот модел ние искористивме корпус кој е достапен на интернет, секако со дозвола на авторите кои можите да ги најдите <a href="#благодарност-до">тука</a>.</p>

<p>Податоците за тренирање ги зачувавме во формат наречен <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000533.shtml">Tab-Separated Value</a> или <b>TSV</b> на кратко, со помош на библиотеката pandas во Python.</p>

<p>Во <b>првата колона ги ставивме речениците на македонски јазик</b>, во <b>втората колона ги ставивме преведените реченици на англиски јазик</b>, дел од речениците беа преведени од почеток, остатокот од другите користевме Google Translate и локални LLM модели со техника на дестилација да враќа формат кој е прифатлив.</p>

<p>Валидација правевме со неколку примероци за квалитетот. Но дефинитивно сметаме дека не е најдобар начин за превод.</p>

<p>Како резултат добивме релативно мал data set од <b>480 илјади преведени реченици</b>.</p>

<p>Дистрибуција според должина на реченици:</p>

<p>Со помош на овие македонско-англиски парови го <a href="2025-06-06-transformer-mk-en#Тренирање:">трениравме моделот</a>.</p>

<h2 id="8-positional-embeddings">8. Positional embeddings</h2>

<p>Оргиналната имплементација користи статични (фиксни) позициони вградувања.</p>

<p>За да се пресмета вредноста на едео позиционо вградување (3.5 во оригиналното истражувње).</p>

<p>Авторите ги користат функциите <b>синус и косинус</b> (наставниците и професорите беа во право, корисни се) :</p>

\[PE(\text{pos}, 2i) = \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\]

\[PE(\text{pos}, 2i+1) = \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\]

<p>Енкодирањето зависи од 3 вредности:</p>

<ul>
  <li>pos - позицијата на векторот</li>
  <li>i - индексот внатре во векторот</li>
  <li>d_model - димензијата на внесот</li>
</ul>

<p>Позиционалните вградувања се користат за информирање на трансфомерот на која позиција се наоѓаат векторите за внес. Тие се додаваат на секоја вредност во векторот посебно,</p>

<h2 id="9-внимание">9. Внимание</h2>

<p><img src="/assets/images/vnimanie.png" alt="Alt text describing the image" /></p>

<p>Концептот на <b>“Внимание”</b> е да го реши проблемот со преведување на текст, пред 20тина години овој проблем бил решаван со комплексни алгоритми кои имале бројни проблеми, наједноставниот проблем била самата <b>должина на речениците при превод</b>, тие се менуваат и стануваат уште по очигледни кога користиме јазици кои имаат различен начин на пишување, за повеќе околу проблемите од класичните начини на превод без корисење на неуронски мрежи можи да прочитате <a href="https://en.wikipedia.org/wiki/Statistical_machine_translation#Shortcomings">тука</a>.</p>

<h3 id="клучеви-вредности">Клучеви, Вредности</h3>

\[Vnimanie(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V\]

<div style="background-color: #ff0000; color: #ffffff; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️  Предупредување:</strong>
    <br />
    <br />
    Моделот е лиценциран под
    <a href="https://www.creativecommons.org/licenses/by-nc/4.0/deed.en" target="_blank" style="color: #ffffff; text-decoration: underline;">
        Creative Commons Attribution Non Commercial 4.0
    </a>
    <br />
    <br />
    Моделот <b>МОЖЕ</b> да се користи за едукативни цели и истражувачки цели.
    <br />
    <br />
    Моделот <b>НЕ</b> може да биде користен во комерцијални продукти или цели на кој било начин.
</div>

<h2 id="sentencepiece">SentencePiece:</h2>

<h3 id="unigram">Unigram:</h3>

<p>е алгоритам за токенизација на дел од зборови, каде што се претпоставува дека веројатноста дека токен ќе се појави не зависи од други токени кој се појави претходно.</p>

<hr />

<p><br /></p>

<h2 id="тренирање">Тренирање:</h2>

<p>Под процесот тренирање се мисли учење на невронската мрежа (трансформерот) да преведува текст.</p>

<p>За овој чекор искористивме <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/">Nvidia графичка RTX 4090</a> со 24 GB VRAM.</p>

<p>Моделот го трениравме 20 епохи, секоја епоха траеше околу еден час, после 20 епохи учење, моделот започна да стагнира.</p>

<p>Кодот за тренирање на моделот може да го најдите тука:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">'is the syntax working?'</span><span class="p">)</span></code></pre></figure>

<p><br /></p>

<hr />

<h2 id="благодарност-до">Благодарност до:</h2>

<p>Би сакале да изразиме посебна благодарност на следниве личности и институции, кои безусловно ни помогнаа и подржаја, без нив проектов не би постоел, од бесценети ресурси по македонски јазик, од песни, книги па се до вести, нашиот модел не би научил да преведи ниту една реченица, затоа би сакале да им се заблагодариме на следниве личности и институции:</p>

<ul>
  <li><a href="https://fikt.uklo.edu.mk/prof-d-r-kostandina-veljanovski/">Проф д-р Костандина Вељановска - ФИКТ - Битола</a>.</li>
  <li><a href="https://fikt.uklo.edu.mk/darko-pajkovski/">М-р Дарко Пајковски - Асистент - ФИКТ - Битола</a>.</li>
  <li><a href="https://drmj.manu.edu.mk/">Акад. Марјан Марковиќ - Дигитални ресурси на македонскиот јазик</a>.</li>
  <li><a href="https://pelister.org/">Проф. д-р Георге Гоце Митревски - pelister.org</a>.</li>
  <li><a href="https://time.mk/trajkovski/">Ph.D Игор Трајковски - time.mk</a>.</li>
  <li><a href="https://pesna.org/">pesna.org</a>.</li>
</ul>

<p><br /></p>

<hr />

<h3 id="литературареференци">Литература/Референци:</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1706.03762">Attention is All You need</a>.</li>
  <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>.</li>
  <li><a href="https://erdem.pl/2021/05/introduction-to-attention-mechanism">Kemal Erdem, (May 2021). “Introduction to Attention Mechanism”</a></li>
  <li>[мал дел од dataset - ]</li>
</ul>

<p><br /></p>

<hr />

<h2 id="корисна-литература-за-почетници-и-за-љубопитните">Корисна литература за почетници и за љубопитните:</h2>

<p>Следниве ресурси се комплетно бесплатни и корисни за сите оние кои сакаат да научат кодирање, машинско учење, линеарна алгебра,веројатност и статистика.</p>

<p>Зошто овие теми, бидејќи со помош на овие теми и вие ќе можите да тренирате и да ги разбирате невронските мрежи и вештачката интелигенција.</p>

<ul>
  <li><a href="https://hefferon.net/linearalgebra/">Линеарна Алгебра - Jim Hefferon - книга кој авторот ја нуди бесплатно</a>.</li>
  <li><a href="https://rentry.org/machine-learning-roadmap">Целосен Roadmap за учење</a>.</li>
  <li><a href="https://ocw.mit.edu/courses/6-100l-introduction-to-cs-and-programming-using-python-fall-2022/pages/material-by-lecture/">Научете Python од <b>MIT</b></a>.</li>
  <li><a href="https://codeinplace.stanford.edu/">Научете Python Интеракивно - Курс од <b>Stanford</b></a>.</li>
  <li><a href="https://cs50.harvard.edu/python/2022/weeks/0/">Научете Python - Курс од <b>Harvard</b></a>.</li>
  <li><a href="https://immersivemath.com/ila/index.html">Научете <b>Линеарна Алгебра визуелно</b>, делот со операции за вектори и матрици е многу важен во машинското учење - <b>Immersive Math</b></a>.</li>
  <li><a href="https://playground.tensorflow.org">Визуелно експериментирање со <b> Tensorflow Deep Playground</b></a>.</li>
  <li><a href="https://www.3blue1brown.com/topics/neural-networks"><b>3b1b</b> - Невронски Мрежи</a>.</li>
  <li><a href="https://youtu.be/qBigTkBLU6g?si=oTADX-oND-uB_FVA"><b>StatQuest</b> - Статистика</a>.</li>
</ul>]]></content><author><name>Stefan Najdovski, Hristijan Gorkov</name></author><category term="basics" /><summary type="html"><![CDATA[]]></summary></entry></feed>