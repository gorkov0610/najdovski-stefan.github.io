---
layout: post
title:  "Имплементациjа на Трансформер Архитектурата за Македонско-Англиски Превод На Реченици"
date:   2025-06-06 09:32:32 +0200
categories: basics
---

<br>

---

# Содржина

0. [Вовед](#0-вовед)
1. [Што е токен?](#1-токен)
2. Речник
3. Токенизирање
4. SentencePiece
5. Token Eembeddings
6. Unigram Model
7. Position Eembedding
8. Внимание
9. Transformer
10. Multi-Head Attention
11. Тренирање
12. Резултати
13. Референци и Благодарност

---

<br>

## 0. Вовед


Целта на овој блог е да научиме како функционира <b>Transformer</b> архитектурата, делот кој ја направи науката за вештачки неуронски мрежи посериозна, практична и достапна за секого, најголема примена има во Large Language Models како GPT, Claude, Mistral, LLAMA...

На крајот има и <b>"demo"</b> од проектот, што може да се користи практично.

Секако нашата имплементација тука ќе биде <b>многу пати поедноставна </b>, но есенцијално идејата е иста, скоро целосно базирана врз оригиналниот труд [Attention is All You Need](https://arxiv.org/pdf/1706.03762).

Целта на нашиот модел ќе биди <b>превод на текст</b>.

Целта <b>нема</b> да биди:  Generative Pretrained Transformer (GPT), имплементација на целосен Large Language Model (LLM), ова е далеку покомплицирано и уште потешко за имплеметнација.

Она што ќе го имплементираме, ќе биди:

<b>Sequence to Sequence Vanilla Transformer</b> или на кратко <b>Seq2Seq Transformer</b>.

Ќе гледаме да балансираме со технички жаргони и да објасниме интуитивно и со примери за секој да може да не следи, ви препорачуваме да имате некоја блага основа за полесно следење, за тоа што е <b>Neural Network</b> и што е тоа <b>Natural Language Processing</b>, исто препорачливо е познавање на <b>основи на веројатност и статистика</b>.


<br>

### Sequence to Sequence


[Seq2Seq]() се користи за обработка на природни јазици [NLP](https://mk.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%BF%D1%80%D0%B8%D1%80%D0%BE%D0%B4%D0%BD%D0%B8_%D1%98%D0%B0%D0%B7%D0%B8%D1%86%D0%B8).

Во нашиот случај ќе го користиме за <b>превод од македонски на англиски</b>, мислиме дека е добар баланс помеѓу нешто што е <b> корисно да направиме за нашиот мајчин јазик</b> ,нешто што <b> не е само теорија</b> и нешто што <b>може да се научи</b>, три во едно :)

Пред да се појави трансформер архитектурата, <b>механизмите за "внимание"</b> биле ограничени со [GRU]() или [LSTM]() и користењето на [RNN-Recurrent Neural Networks]().




<div style="background-color: #fff3cd; color: #856404; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️ Мало Предупредување:</strong>

    <br>
    <br>

        Овој блог е пишуван од студенти, сеуште ги проучуваме детално сите идеи презентирани на блогот

  <br>
  <br>

  Сигурни сме дека имаме некаде грешка. Очекувајте грешки како граматички и нестандарден јазик.

  <br>
  <br>

  <b>Отворени сме на конструктивни критики</b>

  <br>
  <br>
  Доколку пронајдите какви било грешки слободно контактирајте не.

  <br>
  <br>
  Благодариме :)
</div>


## 1. Токен

Овој збор кај нас би бил преведен како <b>лексема</b> или <b>жетон</b>, во англиската (програмерската) литература e дефиниран како <b>атомична (неделива) единица за репрезентација нa текст.</b> (искрено не сме сигурни дали го имаат истото значење на македонски или општо во лингвистиката).

Должината на оваа единица е <b>произволна</b> и зависи од проблемот што сакаме да го решиме.

Се сретнува во повеќе должини:
  - <b>Реченица</b> (пример: Здраво Македонијо!)
  - <b>Дел од збор</b> (пример во вистински јазик би биле слоговите).
  - <b>Збор</b>(пример: <b>здраво</b>).
  - <b>Карактер</b> (пример: <b>а</b>).
  - <b>Бајт</b> (пример: <b>ASCII</b> или <b>UTF</b> енкодиран карактер).

Ние ќе ги користиме Unigram Моделот.

<br>

## 3.Речник

## 9.Внимание

Концептот на <b>"Внимание"</b> е да го реши проблемот со преведување на текст, пред 20тина години овој проблем бил решаван со комплексни алгоритми кои имале бројни проблеми, наједноставниот проблем била самата <b>должина на речениците при превод</b>, тие се менуваат и стануваат уште по очигледни кога користиме јазици кои имаат различен начин на пишување, за повеќе околу проблемите од класичните начини на превод без корисење на неуронски мрежи можи да прочитате [тука](https://en.wikipedia.org/wiki/Statistical_machine_translation#Shortcomings).



<div style="background-color: #ff0000; color: #ffffff; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️  Предупредување:</strong>
    <br>
    Моделот е лиценциран под
    <a href="https://www.creativecommons.org/licenses/by-nc/4.0/deed.en" target="_blank" style="color: #ffffff; text-decoration: underline;">
        Creative Commons Attribution Non Commercial 4.0
    </a>
    <br>
    <br>
    Моделот <b>МОЖЕ</b> да се користи за едукативни цели и истражувачки цели.
    <br>
    <br>
    Моделот <b>НЕ</b> може да биде користен во комерцијални продукти или цели на кој било начин.
</div>

<!--
{% highlight python %}
print('is the syntax working?')
{% endhighlight %} -->
