---
layout: post
title:  "Имплементациjа на Трансформер Архитектурата за Македонско-Англиски Превод На Реченици"
date:   2025-06-06 09:32:32 +0200
categories: basics
---

# Содржина

0. Вовед
1. Што е токен?
2. Речник
3. Токенизирање
4. SentencePiece
5. Token Eembeddings
6. Position Eembedding
7. Внимание
8. Transformer
9. Multi-Head Attention
10. Тренирање
11. Резултати
12. Референци и Благодарност

---

<br>

## 0. Вовед


Целта на овој блог е да научиме како функционира <b>Transformer</b> архитектурата, која се користи во Large Language Models како GPT, Claude, Mistral, LLAMA...

Секако нашата имплементација тука ќе биде <b>многу пати поедноставна </b>, примарно базирана врз оригиналниот труд [Attention is All You Need]().

Исто целта <b>нема</b> да биди Generative Pretrained Transformer или имплементација на цел LLM туку со помош <b>Sequence to Sequence Vanilla Transformer</b> ќе можиме да преведуваме текст.

<div style="background-color: #fff3cd; color: #856404; border: 1px solid #ffeeba; padding: 1rem; border-radius: 6px; margin: 1rem 0;">
    <strong>⚠️ Мало Предупредување:</strong>

    <br>
    <br>

        Овој блог е пишуван од студенти, сеуште го проучуваме сето ова на блогот.

  <br>
  <br>

  Сигурни сме дека имаме некаде грешка. Очекувајте грешки како граматички и нестандарден јазик.

  <br>
  <br>

  <b>Отворени сме на конструктивни критики</b>

  <br>
  <br>
  Доколку пронајдите какви било грешки слободно контактирајте не.

  <br>
  <br>
  Благодариме :)
</div>


[Seq2Seq]() се користи за обработка на природни јазици [NLP](https://mk.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%BD%D0%B0_%D0%BF%D1%80%D0%B8%D1%80%D0%BE%D0%B4%D0%BD%D0%B8_%D1%98%D0%B0%D0%B7%D0%B8%D1%86%D0%B8).

Во нашиот случај ќе го користиме за <b>превод од македонски на англиски</b>, мислиме дека е добар баланс помеѓу нешто што е <b> корисно да направиме за нашиот мајчин јазик</b> ,нешто што <b> не е само теорија</b> и нешто што <b>може да се научи</b>, три во едно :)

Пред да се појави трансформер архитектурата, <b>механизмите за "внимание"</b> биле ограничени со [GRU]() или [LSTM]() и користењето на [RNN-Recurrent Neural Networks]().


## 1. Токен

Овој збор кај нас би бил преведен како <b>лексема</b> или <b>жетон</b>, во англиската литература e дефиниран како <b>атомична (неделива) единица за репрезентација нa текст.</b> (искрено не сме сигурни дали го имаат истото значење).

Должината на оваа единица е <b>произволна</b> и зависи од проблемот што сакаме да го решиме.

Се сретнува во повеќе должини:
  - како <b>дел од збор</b> (пример во вистински јазик би биле слоговите).
  - Како <b>збор</b>(пример: <b>здраво</b>).
  - како <b>карактер</b> (пример: <b>а</b>).
  - како <b>Бајт</b> (пример: <b>ASCII</b> или <b>UTF</b> енкодиран карактер).

<br>

### Токен ИД (Token ID)



<!--
{% highlight python %}
print('is the syntax working?')
{% endhighlight %} -->
